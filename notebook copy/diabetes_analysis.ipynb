{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a3f7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "050b2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Diabetes\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"diabetes_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab07f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "cols_to_drop = [\n",
    "    \"ethnicity\",\n",
    "    \"gender\",\n",
    "    \"education_level\",\n",
    "    \"smoking_status\",\n",
    "    \"alcohol_consumption_per_week\",\n",
    "    \"sleep_hours_per_day\",\n",
    "    \"screen_time_hours_per_day\",\n",
    "    \"hypertension_history\",\n",
    "    \"cardiovascular_history\",\n",
    "    \"waist_to_hip_ratio\",\n",
    "    \"diastolic_bp\",\n",
    "    \"heart_rate\",\n",
    "    \"cholesterol_total\",\n",
    "    \"ldl_cholesterol\",\n",
    "    \"glucose_postprandial\",\n",
    "    \"insulin_level\",\n",
    "    \"income_level\",\n",
    "    \"employment_status\",\n",
    "    \"diabetes_stage\",\n",
    "    \"diagnosed_diabetes\"\n",
    "]\n",
    "\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f4a60da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+----------+-----------------------+---+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "|age|physical_activity_minutes_per_week|diet_score|family_history_diabetes|bmi|systolic_bp|hdl_cholesterol|triglycerides|glucose_fasting|hba1c|diabetes_risk_score|\n",
      "+---+----------------------------------+----------+-----------------------+---+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "|  0|                                 0|         0|                      0|  0|          0|              0|            0|              0|    0|                  0|\n",
      "+---+----------------------------------+----------+-----------------------+---+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Kiểm tra dữ liệu bị thiếu\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fda91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 100000\n",
      "Distinct rows: 100000\n",
      "Duplicate rows: 0\n",
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra dữ liệu trùng\n",
    "total = df.count()\n",
    "distinct_all = df.dropDuplicates().count()\n",
    "duplicates_all = total - distinct_all\n",
    "print(f\"Total rows: {total}\")\n",
    "print(f\"Distinct rows: {distinct_all}\")\n",
    "print(f\"Duplicate rows: {duplicates_all}\")\n",
    "if duplicates_all > 0:\n",
    "    from pyspark.sql.functions import col\n",
    "    # Group by all columns to find groups with count>1 (may be expensive on large datasets)\n",
    "    dup_groups = df.groupBy(df.columns).count().filter(col(\"count\") > 1).orderBy(col(\"count\").desc())\n",
    "    print(\"Duplicate groups:\")\n",
    "    dup_groups.show(10, truncate=False)\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a72ad592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmi: 809 outliers\n",
      "age: 0 outliers\n",
      "physical_activity_minutes_per_week: 3352 outliers\n",
      "systolic_bp: 360 outliers\n",
      "triglycerides: 389 outliers\n",
      "glucose_fasting: 745 outliers\n",
      "hba1c: 687 outliers\n",
      "diabetes_risk_score: 1182 outliers\n"
     ]
    }
   ],
   "source": [
    "outlier_cols = [\n",
    "    \"bmi\",\n",
    "    \"age\",\n",
    "    \"physical_activity_minutes_per_week\",\n",
    "    \"systolic_bp\",\n",
    "    \"triglycerides\",\n",
    "    \"glucose_fasting\",\n",
    "    \"hba1c\",\n",
    "    \"diabetes_risk_score\"\n",
    "]\n",
    "\n",
    "for col_name in outlier_cols:\n",
    "    q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    \n",
    "    count_outlier = df.filter(\n",
    "        (df[col_name] < lower) | (df[col_name] > upper)\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"{col_name}: {count_outlier} outliers\")\n",
    "\n",
    "    pdf = df.select(outlier_cols).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f5bfee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+----------+-----------------------+----+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "|age|physical_activity_minutes_per_week|diet_score|family_history_diabetes| bmi|systolic_bp|hdl_cholesterol|triglycerides|glucose_fasting|hba1c|diabetes_risk_score|\n",
      "+---+----------------------------------+----------+-----------------------+----+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "| 58|                               215|       5.7|                      0|30.5|        134|             41|          145|            136| 8.18|               29.6|\n",
      "| 48|                               143|       6.7|                      0|23.1|        129|             55|           30|             93| 5.63|               23.0|\n",
      "| 60|                                57|       6.4|                      1|22.2|        115|             66|           36|            118| 7.51|               44.7|\n",
      "| 74|                                49|       3.4|                      0|26.8|        120|             50|          140|            139| 9.03|               38.2|\n",
      "| 46|                               109|       7.2|                      0|21.2|         92|             52|          160|            137|  7.2|               23.5|\n",
      "| 46|                               124|       9.0|                      0|26.1|         95|             61|          179|            100| 6.03|               23.5|\n",
      "| 75|                                53|       9.2|                      0|25.1|        129|             46|          155|            101| 5.24|               36.1|\n",
      "| 62|                                75|       4.1|                      0|23.9|        128|             49|          120|            110| 7.04|               34.2|\n",
      "| 42|                               114|       6.7|                      0|24.7|        103|             33|           98|            116|  6.9|               26.7|\n",
      "| 59|                                86|       8.2|                      0|26.7|        124|             52|          104|             76| 4.99|               30.0|\n",
      "| 43|                               118|       7.5|                      0|24.8|        109|             44|          182|            124| 7.34|               25.3|\n",
      "| 43|                               167|       7.2|                      0|22.3|        119|             59|           49|            117|  7.4|               18.5|\n",
      "| 54|                                13|       6.7|                      0|31.2|        120|             40|          222|            123| 6.88|               37.5|\n",
      "| 19|                               364|       5.7|                      0|21.6|        105|             64|          141|             83| 5.59|                7.3|\n",
      "| 22|                               105|       7.0|                      0|31.4|        113|             47|          131|            105| 5.88|               21.1|\n",
      "| 41|                                99|       6.6|                      0|25.9|        118|             54|          140|            110| 6.05|               24.1|\n",
      "| 34|                                31|       5.3|                      0|20.3|        107|             56|           73|            105| 6.88|               23.1|\n",
      "| 55|                                54|       3.4|                      0|24.8|        110|             52|          180|            100| 5.89|               32.8|\n",
      "| 35|                                14|       5.4|                      0|18.0|        100|             40|          148|            101| 6.61|               26.1|\n",
      "| 27|                               164|       8.6|                      0|22.8|         99|             61|          148|             91| 6.23|               16.0|\n",
      "+---+----------------------------------+----------+-----------------------+----+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66f27c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: output/cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Export cleaned DataFrame as single CSV file\n",
    "import os, glob, shutil\n",
    "out_dir = \"output/cleaned_data_tmp\"\n",
    "# Coalesce to a single partition and write out\n",
    "df.coalesce(1).write.csv(out_dir, header=True, mode=\"overwrite\")\n",
    "# Find the single part file Spark produced and move it\n",
    "part_files = glob.glob(os.path.join(out_dir, \"part-*.csv\"))\n",
    "if part_files:\n",
    "    part_file = part_files[0]\n",
    "    dest = \"output/cleaned_data.csv\"\n",
    "    shutil.move(part_file, dest)\n",
    "    # cleanup temporary folder contents except the moved file\n",
    "    for f in glob.glob(os.path.join(out_dir, \"*\")):\n",
    "        # skip if it's the moved file (already relocated)\n",
    "        if os.path.abspath(f) == os.path.abspath(dest):\n",
    "            continue\n",
    "        if os.path.isdir(f):\n",
    "            shutil.rmtree(f, ignore_errors=True)\n",
    "        else:\n",
    "            try:\n",
    "                os.remove(f)\n",
    "            except OSError:\n",
    "                pass\n",
    "    try:\n",
    "        os.rmdir(out_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print(\"Written:\", dest)\n",
    "else:\n",
    "    print(\"No part file found in\", out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
