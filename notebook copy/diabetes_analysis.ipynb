{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3f7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "050b2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Diabetes\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"diabetes_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f4a60da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---------------+------------+-----------------+--------------+----------------------------+----------------------------------+----------+-------------------+-------------------------+-----------------------+--------------------+----------------------+---+------------------+-----------+------------+----------+-----------------+---------------+---------------+-------------+---------------+--------------------+-------------+-----+-------------------+--------------+------------------+\n",
      "|age|gender|ethnicity|education_level|income_level|employment_status|smoking_status|alcohol_consumption_per_week|physical_activity_minutes_per_week|diet_score|sleep_hours_per_day|screen_time_hours_per_day|family_history_diabetes|hypertension_history|cardiovascular_history|bmi|waist_to_hip_ratio|systolic_bp|diastolic_bp|heart_rate|cholesterol_total|hdl_cholesterol|ldl_cholesterol|triglycerides|glucose_fasting|glucose_postprandial|insulin_level|hba1c|diabetes_risk_score|diabetes_stage|diagnosed_diabetes|\n",
      "+---+------+---------+---------------+------------+-----------------+--------------+----------------------------+----------------------------------+----------+-------------------+-------------------------+-----------------------+--------------------+----------------------+---+------------------+-----------+------------+----------+-----------------+---------------+---------------+-------------+---------------+--------------------+-------------+-----+-------------------+--------------+------------------+\n",
      "|  0|     0|        0|              0|           0|                0|             0|                           0|                                 0|         0|                  0|                        0|                      0|                   0|                     0|  0|                 0|          0|           0|         0|                0|              0|              0|            0|              0|                   0|            0|    0|                  0|             0|                 0|\n",
      "+---+------+---------+---------------+------------+-----------------+--------------+----------------------------+----------------------------------+----------+-------------------+-------------------------+-----------------------+--------------------+----------------------+---+------------------+-----------+------------+----------+-----------------+---------------+---------------+-------------+---------------+--------------------+-------------+-----+-------------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check null\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9fda91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 100000\n",
      "Distinct rows: 100000\n",
      "Duplicate rows: 0\n",
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "# Check duplicates\n",
    "# This cell computes total rows, distinct rows (all columns), and shows sample duplicate groups\n",
    "total = df.count()\n",
    "distinct_all = df.dropDuplicates().count()\n",
    "duplicates_all = total - distinct_all\n",
    "print(f\"Total rows: {total}\")\n",
    "print(f\"Distinct rows: {distinct_all}\")\n",
    "print(f\"Duplicate rows: {duplicates_all}\")\n",
    "if duplicates_all > 0:\n",
    "    from pyspark.sql.functions import col\n",
    "    # Group by all columns to find groups with count>1 (may be expensive on large datasets)\n",
    "    dup_groups = df.groupBy(df.columns).count().filter(col(\"count\") > 1).orderBy(col(\"count\").desc())\n",
    "    print(\"Duplicate groups:\")\n",
    "    dup_groups.show(10, truncate=False)\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab07f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "cols_to_drop = [\n",
    "    \"ethnicity\",\n",
    "    \"education_level\",\n",
    "    \"smoking_status\",\n",
    "    \"alcohol_consumption_per_week\",\n",
    "    \"sleep_hours_per_day\",\n",
    "    \"screen_time_hours_per_day\",\n",
    "    \"hypertension_history\",\n",
    "    \"cardiovascular_history\",\n",
    "    \"waist_to_hip_ratio\",\n",
    "    \"diastolic_bp\",\n",
    "    \"heart_rate\",\n",
    "    \"cholesterol_total\",\n",
    "    \"ldl_cholesterol\",\n",
    "    \"glucose_postprandial\",\n",
    "    \"insulin_level\",\n",
    "    \"income_level\",\n",
    "    \"employment_status\",\n",
    "    \"diabetes_stage\",\n",
    "    \"diagnosed_diabetes\"\n",
    "]\n",
    "\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03a8fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     1|\n",
      "|     0|\n",
      "|     1|\n",
      "|     0|\n",
      "|     1|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- physical_activity_minutes_per_week: integer (nullable = true)\n",
      " |-- diet_score: double (nullable = true)\n",
      " |-- family_history_diabetes: integer (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- systolic_bp: integer (nullable = true)\n",
      " |-- hdl_cholesterol: integer (nullable = true)\n",
      " |-- triglycerides: integer (nullable = true)\n",
      " |-- glucose_fasting: integer (nullable = true)\n",
      " |-- hba1c: double (nullable = true)\n",
      " |-- diabetes_risk_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"gender\",\n",
    "    when(col(\"gender\") == \"Male\", 1)\n",
    "    .when(col(\"gender\") == \"Female\", 0)\n",
    "    .otherwise(None)   # nếu có giá trị lạ thì để null\n",
    ")\n",
    "\n",
    "df.select(\"gender\").show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f5bfee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------------------------+----------+-----------------------+----+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "|age|gender|physical_activity_minutes_per_week|diet_score|family_history_diabetes| bmi|systolic_bp|hdl_cholesterol|triglycerides|glucose_fasting|hba1c|diabetes_risk_score|\n",
      "+---+------+----------------------------------+----------+-----------------------+----+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "| 58|     1|                               215|       5.7|                      0|30.5|        134|             41|          145|            136| 8.18|               29.6|\n",
      "| 48|     0|                               143|       6.7|                      0|23.1|        129|             55|           30|             93| 5.63|               23.0|\n",
      "| 60|     1|                                57|       6.4|                      1|22.2|        115|             66|           36|            118| 7.51|               44.7|\n",
      "| 74|     0|                                49|       3.4|                      0|26.8|        120|             50|          140|            139| 9.03|               38.2|\n",
      "| 46|     1|                               109|       7.2|                      0|21.2|         92|             52|          160|            137|  7.2|               23.5|\n",
      "| 46|     0|                               124|       9.0|                      0|26.1|         95|             61|          179|            100| 6.03|               23.5|\n",
      "| 75|     0|                                53|       9.2|                      0|25.1|        129|             46|          155|            101| 5.24|               36.1|\n",
      "| 62|     1|                                75|       4.1|                      0|23.9|        128|             49|          120|            110| 7.04|               34.2|\n",
      "| 42|     1|                               114|       6.7|                      0|24.7|        103|             33|           98|            116|  6.9|               26.7|\n",
      "| 59|     0|                                86|       8.2|                      0|26.7|        124|             52|          104|             76| 4.99|               30.0|\n",
      "| 43|     0|                               118|       7.5|                      0|24.8|        109|             44|          182|            124| 7.34|               25.3|\n",
      "| 43|     0|                               167|       7.2|                      0|22.3|        119|             59|           49|            117|  7.4|               18.5|\n",
      "| 54|     0|                                13|       6.7|                      0|31.2|        120|             40|          222|            123| 6.88|               37.5|\n",
      "| 19|     1|                               364|       5.7|                      0|21.6|        105|             64|          141|             83| 5.59|                7.3|\n",
      "| 22|     1|                               105|       7.0|                      0|31.4|        113|             47|          131|            105| 5.88|               21.1|\n",
      "| 41|     1|                                99|       6.6|                      0|25.9|        118|             54|          140|            110| 6.05|               24.1|\n",
      "| 34|     1|                                31|       5.3|                      0|20.3|        107|             56|           73|            105| 6.88|               23.1|\n",
      "| 55|     0|                                54|       3.4|                      0|24.8|        110|             52|          180|            100| 5.89|               32.8|\n",
      "| 35|     1|                                14|       5.4|                      0|18.0|        100|             40|          148|            101| 6.61|               26.1|\n",
      "| 27|     1|                               164|       8.6|                      0|22.8|         99|             61|          148|             91| 6.23|               16.0|\n",
      "+---+------+----------------------------------+----------+-----------------------+----+-----------+---------------+-------------+---------------+-----+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66f27c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: output/cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Export cleaned DataFrame as single CSV file\n",
    "import os, glob, shutil\n",
    "out_dir = \"output/cleaned_data_tmp\"\n",
    "# Coalesce to a single partition and write out\n",
    "df.coalesce(1).write.csv(out_dir, header=True, mode=\"overwrite\")\n",
    "# Find the single part file Spark produced and move it\n",
    "part_files = glob.glob(os.path.join(out_dir, \"part-*.csv\"))\n",
    "if part_files:\n",
    "    part_file = part_files[0]\n",
    "    dest = \"output/cleaned_data.csv\"\n",
    "    shutil.move(part_file, dest)\n",
    "    # cleanup temporary folder contents except the moved file\n",
    "    for f in glob.glob(os.path.join(out_dir, \"*\")):\n",
    "        # skip if it's the moved file (already relocated)\n",
    "        if os.path.abspath(f) == os.path.abspath(dest):\n",
    "            continue\n",
    "        if os.path.isdir(f):\n",
    "            shutil.rmtree(f, ignore_errors=True)\n",
    "        else:\n",
    "            try:\n",
    "                os.remove(f)\n",
    "            except OSError:\n",
    "                pass\n",
    "    try:\n",
    "        os.rmdir(out_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print(\"Written:\", dest)\n",
    "else:\n",
    "    print(\"No part file found in\", out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
